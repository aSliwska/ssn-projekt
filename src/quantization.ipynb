{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15b25d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision.datasets import CIFAR10\n",
    "import torch.nn as nn\n",
    "from torchvision.transforms import v2\n",
    "import multiprocessing\n",
    "import torch.quantization\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621f91c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run variables\n",
    "\n",
    "seed = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c01e31f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda device\n"
     ]
    }
   ],
   "source": [
    "# device settings\n",
    "\n",
    "num_workers = multiprocessing.cpu_count() // 2\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bd19ef74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barte\\miniconda3\\Lib\\site-packages\\torchvision\\transforms\\v2\\_deprecated.py:42: UserWarning: The transform `ToTensor()` is deprecated and will be removed in a future release. Instead, please use `v2.Compose([v2.ToImage(), v2.ToDtype(torch.float32, scale=True)])`.Output is equivalent up to float precision.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train set size: 40000\n",
      "validation set size: 10000\n",
      "test set size: 10000\n"
     ]
    }
   ],
   "source": [
    "# define datasets and loaders\n",
    "\n",
    "transform = v2.Compose([\n",
    "    v2.ToTensor(),  \n",
    "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = CIFAR10(\"./../data\", train=True, transform=transform, download=True)\n",
    "test_dataset = CIFAR10(\"./../data\", train=False, transform=transform, download=True)\n",
    "\n",
    "train_dataset, validation_dataset =  random_split(train_dataset, [0.8, 0.2])\n",
    "\n",
    "print('train set size:', len(train_dataset))\n",
    "print('validation set size:', len(validation_dataset))\n",
    "print('test set size:', len(test_dataset))\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True, num_workers=num_workers)\n",
    "validation_loader = DataLoader(validation_dataset, batch_size=128, shuffle=True, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "class_names = [\"airplane\", \"automobile\", \"bird\", \"cat\", \"deer\", \"dog\", \"frog\", \"horse\", \"ship\", \"truck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3487366f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseNN(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(BaseNN, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(2048, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.1),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.features(x)\n",
    "        x = x.reshape(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "        x = self.dequant(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14b30b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = BaseNN(num_classes=10).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb82ada",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs, learning_rate):\n",
    "    trainingEpoch_loss = []\n",
    "    validationEpoch_loss = []\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    qat_started = False \n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "\n",
    "        if epoch == 2 and not qat_started:\n",
    "            print(\"Rozpoczynanie Quantization-Aware Training (QAT)...\")\n",
    "            model.qconfig = torch.quantization.get_default_qat_qconfig(\"fbgemm\")\n",
    "            torch.quantization.prepare_qat(model, inplace=True)\n",
    "            qat_started = True\n",
    "\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        trainingEpoch_loss.append(avg_train_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Training Loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "        # Ewaluacja\n",
    "        model.eval()\n",
    "        validation_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in validation_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "                loss = criterion(logits, labels)\n",
    "\n",
    "                validation_loss += loss.item()\n",
    "\n",
    "        avg_val_loss = validation_loss / len(validation_loader)\n",
    "        validationEpoch_loss.append(avg_val_loss)\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Validation Loss: {avg_val_loss:.4f}\")\n",
    "\n",
    "    return trainingEpoch_loss, validationEpoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bca9a2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model):\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    model.eval()  \n",
    "    \n",
    "    with torch.no_grad():  \n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to(\"cpu\"), labels.to(\"cpu\")  \n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            logits = outputs[0] if isinstance(outputs, tuple) else outputs\n",
    "\n",
    "            _, predicted = torch.max(logits.data, 1)  \n",
    "\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff48faf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/7, Training Loss: 1.3936\n",
      "Epoch 1/7, Validation Loss: 1.1366\n",
      "Epoch 2/7, Training Loss: 0.9849\n",
      "Epoch 2/7, Validation Loss: 0.9388\n",
      "Rozpoczynanie Quantization-Aware Training (QAT)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barte\\miniconda3\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/7, Training Loss: 0.8559\n",
      "Epoch 3/7, Validation Loss: 0.8929\n",
      "Epoch 4/7, Training Loss: 0.7643\n",
      "Epoch 4/7, Validation Loss: 0.8042\n",
      "Epoch 5/7, Training Loss: 0.6980\n",
      "Epoch 5/7, Validation Loss: 0.7770\n",
      "Epoch 6/7, Training Loss: 0.6425\n",
      "Epoch 6/7, Validation Loss: 0.7858\n",
      "Epoch 7/7, Training Loss: 0.5941\n",
      "Epoch 7/7, Validation Loss: 0.7272\n"
     ]
    }
   ],
   "source": [
    "trainingEpoch_loss, validationEpoch_loss = train(model, epochs=7, learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f9d43a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(\"cpu\")\n",
    "model = torch.quantization.convert(model, inplace=True) \n",
    "torch.save(model.state_dict(), \"../models/quantized_model_during_training.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a087aa42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\barte\\AppData\\Local\\Temp\\ipykernel_14888\\2896501321.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  base_model.load_state_dict(torch.load(\"../models/quantized_base_model.pt\"))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model = BaseNN(num_classes=10).to(\"cpu\")\n",
    "base_model.load_state_dict(torch.load(\"../models/quantized_base_model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "75bb68de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\barte\\miniconda3\\Lib\\site-packages\\torch\\ao\\quantization\\observer.py:229: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "base_model.eval()\n",
    "\n",
    "base_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "\n",
    "torch.quantization.prepare(base_model, inplace=True)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i, (inputs, _) in enumerate(train_loader):\n",
    "        if i >= 10: break \n",
    "        base_model(inputs)\n",
    "\n",
    "after_model = torch.quantization.convert(base_model, inplace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e1cdd926",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(after_model.state_dict(), \"../models/quantized_model_after_training.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d5412d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model\n",
      "Test Accuracy: 73.11%\n",
      "Quntized model during training\n",
      "Test Accuracy: 74.37%\n",
      "Delta: 1.2600000000000051\n",
      "Quntized model after training\n",
      "Test Accuracy: 72.68%\n",
      "Delta: -0.4299999999999926\n"
     ]
    }
   ],
   "source": [
    "print(f\"Base model\")\n",
    "acc_base = test(base_model)\n",
    "\n",
    "print(f\"Quntized model during training\")\n",
    "acc_during = test(model)\n",
    "print(f\"Delta: {acc_during - acc_base}\")\n",
    "\n",
    "print(f\"Quntized model after training\")\n",
    "acc_after = test(after_model)\n",
    "print(f\"Delta: {acc_after - acc_base}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "10101c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model size: 4.54 MB\n",
      "Quantized model during training size: 1.17 MB\n",
      "Quantized model after training size: 1.17 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "size_MB_during = os.path.getsize(\"../models/quantized_model_during_training.pt\") / (1024 * 1024)\n",
    "size_MB_base = os.path.getsize(\"../models/quantized_base_model.pt\") / (1024 * 1024)\n",
    "size_MB_after = os.path.getsize(\"../models/quantized_model_after_training.pt\") / (1024 * 1024)\n",
    "print(f\"Base model size: {size_MB_base:.2f} MB\")\n",
    "print(f\"Quantized model during training size: {size_MB_during:.2f} MB\")\n",
    "print(f\"Quantized model after training size: {size_MB_after:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cdb0c294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model: latency: 344.24 ms per inference\n",
      "During training: latency: 148.18 ms per inference\n",
      "After training: latency: 142.37 ms per inference\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "base_model.eval()\n",
    "inputs, labels = next(iter(test_loader))\n",
    "input = inputs.to(\"cpu\")\n",
    "\n",
    "start = time.time()\n",
    "output = base_model(input)\n",
    "end = time.time()\n",
    "\n",
    "latency_ms = (end - start) * 1000\n",
    "\n",
    "print(f\"Base model: latency: {latency_ms:.2f} ms per inference\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "start = time.time()\n",
    "output = model(input)\n",
    "end = time.time()\n",
    "\n",
    "latency_ms = (end - start) * 1000\n",
    "\n",
    "print(f\"During training: latency: {latency_ms:.2f} ms per inference\")\n",
    "\n",
    "after_model.eval()\n",
    "\n",
    "start = time.time()\n",
    "output = after_model(input)\n",
    "end = time.time()\n",
    "\n",
    "latency_ms = (end - start) * 1000\n",
    "\n",
    "print(f\"After training: latency: {latency_ms:.2f} ms per inference\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4cbfcb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 18:20:29] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 18:20:29] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:20:29] [setup] CPU Tracking...\n",
      "[codecarbon WARNING @ 18:20:31] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 18:20:31] CPU Model on constant consumption mode: AMD Ryzen 5 2600 Six-Core Processor\n",
      "[codecarbon WARNING @ 18:20:31] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:20:31] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:20:31] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 18:20:31] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 18:20:31] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:20:31]   Platform system: Windows-11-10.0.26100-SP0\n",
      "[codecarbon INFO @ 18:20:31]   Python version: 3.12.3\n",
      "[codecarbon INFO @ 18:20:31]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 18:20:31]   Available RAM : 15.924 GB\n",
      "[codecarbon INFO @ 18:20:31]   CPU count: 12 thread(s) in 12 physical CPU(s)\n",
      "[codecarbon INFO @ 18:20:31]   CPU model: AMD Ryzen 5 2600 Six-Core Processor\n",
      "[codecarbon INFO @ 18:20:31]   GPU count: 1\n",
      "[codecarbon INFO @ 18:20:31]   GPU model: 1 x NVIDIA GeForce GTX 1650\n",
      "[codecarbon INFO @ 18:20:34] Emissions data (if any) will be saved to file c:\\Users\\barte\\Desktop\\Bartek_stuff\\SSN\\ssn-projekt\\src\\emissions.csv\n",
      "[codecarbon INFO @ 18:20:34] Energy consumed for RAM : 0.000001 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:20:34] Delta energy consumed for CPU with constant : 0.000039 kWh, power : 390.0 W\n",
      "[codecarbon INFO @ 18:20:34] Energy consumed for All CPU : 0.000039 kWh\n",
      "[codecarbon INFO @ 18:20:34] Energy consumed for all GPUs : 0.000001 kWh. Total GPU Power : 15.052732992376633 W\n",
      "[codecarbon INFO @ 18:20:34] 0.000041 kWh of electricity used since the beginning.\n",
      "[codecarbon WARNING @ 18:20:34] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 18:20:34] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:20:34] [setup] CPU Tracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model - Estimated CO2 emissions: 2.7130355684580645e-05 kg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 18:20:36] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 18:20:36] CPU Model on constant consumption mode: AMD Ryzen 5 2600 Six-Core Processor\n",
      "[codecarbon WARNING @ 18:20:36] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:20:36] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:20:36] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 18:20:36] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 18:20:36] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:20:36]   Platform system: Windows-11-10.0.26100-SP0\n",
      "[codecarbon INFO @ 18:20:36]   Python version: 3.12.3\n",
      "[codecarbon INFO @ 18:20:36]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 18:20:36]   Available RAM : 15.924 GB\n",
      "[codecarbon INFO @ 18:20:36]   CPU count: 12 thread(s) in 12 physical CPU(s)\n",
      "[codecarbon INFO @ 18:20:36]   CPU model: AMD Ryzen 5 2600 Six-Core Processor\n",
      "[codecarbon INFO @ 18:20:36]   GPU count: 1\n",
      "[codecarbon INFO @ 18:20:36]   GPU model: 1 x NVIDIA GeForce GTX 1650\n",
      "[codecarbon INFO @ 18:20:39] Emissions data (if any) will be saved to file c:\\Users\\barte\\Desktop\\Bartek_stuff\\SSN\\ssn-projekt\\src\\emissions.csv\n",
      "[codecarbon INFO @ 18:20:40] Energy consumed for RAM : 0.000000 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:20:40] Delta energy consumed for CPU with constant : 0.000016 kWh, power : 390.0 W\n",
      "[codecarbon INFO @ 18:20:40] Energy consumed for All CPU : 0.000016 kWh\n",
      "[codecarbon INFO @ 18:20:40] Energy consumed for all GPUs : 0.000001 kWh. Total GPU Power : 17.121988952154656 W\n",
      "[codecarbon INFO @ 18:20:40] 0.000017 kWh of electricity used since the beginning.\n",
      "[codecarbon WARNING @ 18:20:40] Multiple instances of codecarbon are allowed to run at the same time.\n",
      "[codecarbon INFO @ 18:20:40] [setup] RAM Tracking...\n",
      "[codecarbon INFO @ 18:20:40] [setup] CPU Tracking...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "During model - Estimated CO2 emissions: 1.1106384829562002e-05 kg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[codecarbon WARNING @ 18:20:41] No CPU tracking mode found. Falling back on estimation based on TDP for CPU. \n",
      " Windows OS detected: Please install Intel Power Gadget to measure CPU\n",
      "\n",
      "[codecarbon INFO @ 18:20:41] CPU Model on constant consumption mode: AMD Ryzen 5 2600 Six-Core Processor\n",
      "[codecarbon WARNING @ 18:20:41] No CPU tracking mode found. Falling back on CPU constant mode.\n",
      "[codecarbon INFO @ 18:20:41] [setup] GPU Tracking...\n",
      "[codecarbon INFO @ 18:20:41] Tracking Nvidia GPU via pynvml\n",
      "[codecarbon INFO @ 18:20:41] The below tracking methods have been set up:\n",
      "                RAM Tracking Method: RAM power estimation model\n",
      "                CPU Tracking Method: global constant\n",
      "                GPU Tracking Method: pynvml\n",
      "            \n",
      "[codecarbon INFO @ 18:20:41] >>> Tracker's metadata:\n",
      "[codecarbon INFO @ 18:20:41]   Platform system: Windows-11-10.0.26100-SP0\n",
      "[codecarbon INFO @ 18:20:41]   Python version: 3.12.3\n",
      "[codecarbon INFO @ 18:20:41]   CodeCarbon version: 3.0.2\n",
      "[codecarbon INFO @ 18:20:41]   Available RAM : 15.924 GB\n",
      "[codecarbon INFO @ 18:20:41]   CPU count: 12 thread(s) in 12 physical CPU(s)\n",
      "[codecarbon INFO @ 18:20:41]   CPU model: AMD Ryzen 5 2600 Six-Core Processor\n",
      "[codecarbon INFO @ 18:20:41]   GPU count: 1\n",
      "[codecarbon INFO @ 18:20:41]   GPU model: 1 x NVIDIA GeForce GTX 1650\n",
      "[codecarbon INFO @ 18:20:45] Emissions data (if any) will be saved to file c:\\Users\\barte\\Desktop\\Bartek_stuff\\SSN\\ssn-projekt\\src\\emissions.csv\n",
      "[codecarbon INFO @ 18:20:45] Energy consumed for RAM : 0.000000 kWh. RAM Power : 10.0 W\n",
      "[codecarbon INFO @ 18:20:45] Delta energy consumed for CPU with constant : 0.000018 kWh, power : 390.0 W\n",
      "[codecarbon INFO @ 18:20:45] Energy consumed for All CPU : 0.000018 kWh\n",
      "[codecarbon INFO @ 18:20:45] Energy consumed for all GPUs : 0.000001 kWh. Total GPU Power : 28.177631432407416 W\n",
      "[codecarbon INFO @ 18:20:45] 0.000019 kWh of electricity used since the beginning.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After model - Estimated CO2 emissions: 1.2811165995216807e-05 kg\n"
     ]
    }
   ],
   "source": [
    "from codecarbon import EmissionsTracker\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "output = base_model(input)\n",
    "\n",
    "emissions = tracker.stop()\n",
    "print(f\"Base model - Estimated CO2 emissions: {emissions} kg\")\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "output = model(input)\n",
    "\n",
    "emissions = tracker.stop()\n",
    "print(f\"During model - Estimated CO2 emissions: {emissions} kg\")\n",
    "\n",
    "tracker = EmissionsTracker()\n",
    "tracker.start()\n",
    "\n",
    "output = after_model(input)\n",
    "\n",
    "emissions = tracker.stop()\n",
    "print(f\"After model - Estimated CO2 emissions: {emissions} kg\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
